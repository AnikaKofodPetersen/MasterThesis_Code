{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff208f7",
   "metadata": {},
   "source": [
    "# Add zeroes to embeddings where there is a gap in the alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d077b015",
   "metadata": {},
   "source": [
    "This notebook ensures equal dimensions of the esm and ps embeddings by adding zeroes where there is an alignment gap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e75f740",
   "metadata": {},
   "source": [
    "### Import and load all the necessary modules and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76990c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "import sys, os\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import BiopythonWarning\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ef94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign pdb path\n",
    "pdb_path = \"../1_Structures/pdb-files\"\n",
    "esm_path = \"PCA_reduced/ESM_embeddings/\"\n",
    "ps_path = \"PCA_reduced/PS_embeddings/\"\n",
    "esm_out = \"Gap_inserted/ESM_embeddings/\"\n",
    "ps_out = \"Gap_inserted/PS_embeddings/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059df9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catch warnings, since these are predicted PDBs\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', BiopythonWarning)\n",
    "    \n",
    "    #initialize\n",
    "    total = len(os.listdir(pdb_path))\n",
    "    count = 0\n",
    "\n",
    "    #Save in a dict\n",
    "    pdb_fasta = dict()\n",
    "\n",
    "    #Iterate over pdb files\n",
    "    for pdb in os.listdir(pdb_path):\n",
    "        count += 1\n",
    "        print(f\"Working with pdb {count}/{total}\", end = \"\\r\")\n",
    "        ID = pdb.split(\".\")[0].split(\"_\")[0]\n",
    "    \n",
    "        #Collect fasta sequence\n",
    "        with open(f\"{pdb_path}/{pdb}\",\"r\") as pdb_file:\n",
    "            for record in SeqIO.parse(pdb_file, \"pdb-atom\"):\n",
    "                pdb_fasta[ID] = record.seq\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the original fasta sequences\n",
    "df = pd.read_csv(\"../0_DataPreprocessing/CleanedData.csv\",sep=\",\")\n",
    "ID = df[\"ID\"]\n",
    "seqs = df[\"sequence\"]\n",
    "\n",
    "#Ensure no trailing blank space\n",
    "seqs = [seq.strip() for seq in seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Order original fastas into dict\n",
    "org_fasta = dict()\n",
    "for i, s in zip(ID,seqs):\n",
    "    org_fasta[i] = Seq(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a6502",
   "metadata": {},
   "source": [
    "### Work with one set of files at a time and perform all necessary insertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870162ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize\n",
    "total_changes = 0\n",
    "count = 0\n",
    "total = len(pdb_fasta)\n",
    "    \n",
    "#Iterate through all pdb fasta\n",
    "for i, s in org_fasta.items():\n",
    "    if i in pdb_fasta:\n",
    "        count += 1\n",
    "        print(f\"Working with entry {count}/{total}\", end = \"\\r\")\n",
    "    \n",
    "        #Perform pariwise alignment\n",
    "        alignment =  pairwise2.align.globalxx(s, pdb_fasta[i])\n",
    "        pdb_seq = alignment[0].seqB\n",
    "        fasta_seq = alignment[0].seqA\n",
    "    \n",
    "        #find gap positions\n",
    "        gap = \"-\"\n",
    "        pdb_gap = [pos for pos, char in enumerate(pdb_seq) if char == gap]\n",
    "        fasta_gap = [pos for pos, char in enumerate(fasta_seq) if char == gap]\n",
    "    \n",
    "        #initialize for sanity check\n",
    "        out_embs_ps = torch.empty(0,0)\n",
    "        out_embs_esm = torch.empty(0,0)\n",
    "    \n",
    "        #Check if there are any gaps in the pdb sequence\n",
    "        if len(pdb_gap) != 0:\n",
    "            total_changes += 1\n",
    "            embs = torch.load(ps_path + \"PS_\" + i + \".pt\")\n",
    "            zero = torch.zeros(1,30)\n",
    "            out_embs_ps = torch.empty(0,30)\n",
    "        \n",
    "            #insert zeroes in the gap positions\n",
    "            for column in range(len(embs)):\n",
    "                if column in pdb_gap:\n",
    "                    pdb_gap = pdb_gap[1:]\n",
    "                    insert = torch.unsqueeze(embs[column], dim=0)\n",
    "                    out_embs_ps = torch.cat([out_embs_ps,zero,insert], dim=0)\n",
    "                else:\n",
    "                    insert = torch.unsqueeze(embs[column], dim=0)\n",
    "                    out_embs_ps = torch.cat([out_embs_ps,insert], dim=0)\n",
    "            \n",
    "            # Append zeroes for trailing gaps        \n",
    "            if len(pdb_gap) != 0:\n",
    "                cat_list = [out_embs_ps]\n",
    "                cat_list += [zero]*len(pdb_gap)\n",
    "                out_embs_ps = torch.cat(cat_list, dim = 0)   \n",
    "\n",
    "       \n",
    "        #Check if there are any gaps in the fasta sequence\n",
    "        if len(fasta_gap) != 0:\n",
    "            total_changes += 1\n",
    "            embs = torch.load(esm_path + \"ESM_\" + i + \".pt\")\n",
    "            zero = torch.zeros(1,30)\n",
    "            out_embs_esm = torch.empty(0,30)\n",
    "        \n",
    "            #insert zeroes in the gap positions\n",
    "            for column in range(len(embs)):\n",
    "                if column in fasta_gap:\n",
    "                    fasta_gap = fasta_gap[1:]\n",
    "                    insert = torch.unsqueeze(embs[column], dim=0)\n",
    "                    out_embs_esm = torch.cat([out_embs_esm,zero,insert], dim=0)\n",
    "                else:\n",
    "                    insert = torch.unsqueeze(embs[column], dim=0)\n",
    "                    out_embs_esm = torch.cat([out_embs_esm, insert], dim=0)\n",
    "\n",
    "            # Append zeroes for trailing gaps        \n",
    "            if len(fasta_gap) != 0:\n",
    "                cat_list = [out_embs_esm]\n",
    "                cat_list += [zero]*len(fasta_gap)\n",
    "                out_embs_esm = torch.cat(cat_list, dim = 0)   \n",
    "       \n",
    "        \n",
    "        #Double check that everything has the same dimensions\n",
    "        assert len(pdb_seq) == len(fasta_seq)\n",
    "        org_ps = torch.load(ps_path + \"PS_\" + i + \".pt\")\n",
    "        org_esm = torch.load(esm_path + \"ESM_\" + i + \".pt\")\n",
    "        if out_embs_ps.shape != torch.empty(0,0).shape:\n",
    "            if out_embs_esm.shape != torch.empty(0,0).shape:\n",
    "                assert out_embs_ps.shape == out_embs_esm.shape\n",
    "            else:\n",
    "                assert out_embs_ps.shape == org_esm.shape\n",
    "        elif out_embs_esm.shape != torch.empty(0,0).shape:\n",
    "            assert out_embs_esm.shape == org_ps.shape\n",
    "        else:\n",
    "            assert org_ps.shape == org_esm.shape\n",
    "\n",
    "        # Save the new embeddings with gaps\n",
    "        if out_embs_ps.shape != torch.empty(0,0).shape:\n",
    "            if out_embs_esm.shape != torch.empty(0,0).shape:\n",
    "                assert out_embs_ps.shape == out_embs_esm.shape\n",
    "                emb_path = f'{ps_out}/PS_{i}.pt'\n",
    "                torch.save(out_embs_ps, emb_path)\n",
    "                emb_path = f'{esm_out}/ESM_{i}.pt'\n",
    "                torch.save(out_embs_esm, emb_path)\n",
    "              \n",
    "            else:\n",
    "                assert out_embs_ps.shape == org_esm.shape\n",
    "                emb_path = f'{ps_out}/PS_{i}.pt'\n",
    "                torch.save(out_embs_ps, emb_path)\n",
    "                emb_path = f'{esm_out}/ESM_{i}.pt'\n",
    "                torch.save(org_esm, emb_path)\n",
    "                \n",
    "        elif out_embs_esm.shape != torch.empty(0,0).shape:\n",
    "            assert out_embs_esm.shape == org_ps.shape\n",
    "            emb_path = f'{ps_out}/PS_{i}.pt'\n",
    "            torch.save(org_ps, emb_path)\n",
    "            emb_path = f'{esm_out}/ESM_{i}.pt'\n",
    "            torch.save(out_embs_esm, emb_path)\n",
    "        \n",
    "        else:\n",
    "            assert org_ps.shape == org_esm.shape   \n",
    "            emb_path = f'{ps_out}/PS_{i}.pt'\n",
    "            torch.save(org_ps, emb_path)\n",
    "            emb_path = f'{esm_out}/ESM_{i}.pt'\n",
    "            torch.save(org_esm, emb_path)\n",
    "            \n",
    "               \n",
    "            \n",
    "    #Handle ESM embeddings with no structure (PS embedding)       \n",
    "    else:\n",
    "        esm_embs = torch.load(esm_path + \"ESM_\" + i + \".pt\")\n",
    "        ps_embs = torch.zeros_like(esm_embs)\n",
    "        emb_path = f'{ps_out}/PS_{i}.pt'\n",
    "        torch.save(ps_embs, emb_path)\n",
    "        emb_path = f'{esm_out}/ESM_{i}.pt'\n",
    "        torch.save(esm_embs, emb_path)\n",
    "\n",
    "print(f\"\\n\\ntotal changes: {total_changes}\")\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
