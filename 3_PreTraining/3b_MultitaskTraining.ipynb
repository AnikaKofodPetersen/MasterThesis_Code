{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62cc0946",
   "metadata": {},
   "source": [
    "# Train a multitask model on concatenated embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113edd3",
   "metadata": {},
   "source": [
    "This notebook aims at concatenating the ESM and PS embeddings and perform multitask learning in order to learn solubility patterns through multitask learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d02fc",
   "metadata": {},
   "source": [
    "### Import and initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21595bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Import stuff\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import seaborn as sn\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import pickle\n",
    "\n",
    "#Set a nice figure size\n",
    "plt.rcParams['figure.figsize'] = [20, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60127e3",
   "metadata": {},
   "source": [
    "### Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd63ff7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Only use cpu\n",
    "device = \"cpu\"\n",
    "\n",
    "#Assign embedding folder\n",
    "ESM_EMB_PATH = \"./ESM_embeddings/\"\n",
    "PS_EMB_PATH = \"./PS_embeddings/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35e1167",
   "metadata": {},
   "source": [
    "## Load labels from csv and clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6156bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NESG Normalization function\n",
    "def NESGNormalizeData(data):\n",
    "    if data != 6:\n",
    "        return (data - 0) / (5 - 0)\n",
    "    else:\n",
    "        return 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a8b1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load the data\n",
    "org_df = pd.read_csv(\"../0_DataPreprocessing/CleanedData.csv\", sep=\",\")\n",
    "\n",
    "\n",
    "#Get list of emb IDs\n",
    "emb_id = list(os.listdir(ESM_EMB_PATH))\n",
    "emb_id = [idx.split(\".\")[0].split(\"_\")[-1] for idx in emb_id]\n",
    "\n",
    "\n",
    "#Drop rows without sequence embeddings (should be none)\n",
    "df = org_df[org_df.ID.isin(emb_id)]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#Replace NaN with 9 for later ignore index\n",
    "df.NESG_label.fillna(6, inplace=True)\n",
    "df.PSI_BIO_label.fillna(9, inplace=True)\n",
    "\n",
    "#Make sure it is integers\n",
    "df[\"NESG_label\"] = df[\"NESG_label\"].astype(int)\n",
    "df[\"PSI_BIO_label\"] = df[\"PSI_BIO_label\"].astype(int)\n",
    "\n",
    "#Reverse the Psi-Bio label\n",
    "df[\"PSI_BIO_label\"] = [2 if x==0 else x for x in df[\"PSI_BIO_label\"]]\n",
    "df[\"PSI_BIO_label\"] = [0 if x==1 else x for x in df[\"PSI_BIO_label\"]]\n",
    "df[\"PSI_BIO_label\"] = [1 if x==2 else x for x in df[\"PSI_BIO_label\"]]\n",
    "\n",
    "#Load labels \n",
    "NESG_label = list(df[\"NESG_label\"])\n",
    "psi_bio_label = list(df[\"PSI_BIO_label\"])\n",
    "\n",
    "#Normalize nesg label between 0 and 1\n",
    "norm_NESG_label = [ NESGNormalizeData(x) for x in NESG_label]\n",
    "\n",
    "df[\"norm_NESG_label\"] = norm_NESG_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba19c4e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9a9314",
   "metadata": {},
   "source": [
    "## Load cluster annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d577a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.read_csv(\"./DB_clu_50_id.tsv\", sep=\"\\t\",  header=None)\n",
    "clusters= clusters.rename(columns={0: 'rep', 1 :'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40591659",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b0aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a cluster dictionary\n",
    "cluster_temp_dict = {}\n",
    "cluster_dict = {}\n",
    "count = 0\n",
    "for i, row in clusters.iterrows():\n",
    "    if row[\"rep\"] in cluster_temp_dict:\n",
    "        cluster_dict[row[\"id\"]] = cluster_temp_dict[row[\"rep\"]]\n",
    "    else:\n",
    "        cluster_temp_dict[row[\"rep\"]] = count \n",
    "        count += 1\n",
    "        cluster_dict[row[\"id\"]] = cluster_temp_dict[row[\"rep\"]]\n",
    "        \n",
    "print(f\"Total amount of clusters: {count}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeb217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append cluster info to df\n",
    "clusters = []\n",
    "for i, row in df.iterrows():\n",
    "    name = row[\"ID\"]\n",
    "    clusters.append(cluster_dict[name])\n",
    "df[\"cluster\"] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45bf8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4452c988",
   "metadata": {},
   "source": [
    "## ESM Data load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a923f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load and format embeddings in a dict\n",
    "ESM_embs_dict = dict()     \n",
    "for file in os.listdir(ESM_EMB_PATH):\n",
    "    name = file.split(\".\")[0].split(\"_\")[-1]\n",
    "    if file.endswith(\".pt\") and name in list(df[\"ID\"]):\n",
    "        print (f\"working with file: {file}\", end=\"\\r\")\n",
    "        tensor_in = torch.load(f'{ESM_EMB_PATH}/{file}')\n",
    "        ESM_embs_dict[name] = tensor_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381b0fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Sanity check\n",
    "assert len(ESM_embs_dict) == len(os.listdir(ESM_EMB_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95224740",
   "metadata": {},
   "source": [
    "## Proteinsolver Data load and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58384e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load and format embeddings\n",
    "PS_embs_dict = dict()\n",
    "for file in os.listdir(PS_EMB_PATH):\n",
    "    name = file.split(\".\")[0].split(\"_\")[-1]\n",
    "    if file.endswith(\".pt\") and name in list(df[\"ID\"]):\n",
    "        print (f\"working with file: {file}\", end=\"\\r\")\n",
    "        tensor_in = torch.load(f'{PS_EMB_PATH}/{file}')\n",
    "        PS_embs_dict[name] = tensor_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce04f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check\n",
    "assert len(PS_embs_dict) == len(os.listdir(PS_EMB_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46950730",
   "metadata": {},
   "source": [
    "## Concatenate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb88eb03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Concatenate PS and ESm embeddings\n",
    "cat_embs_dict = dict()\n",
    "\n",
    "# Iterate through sequence embeddings\n",
    "for key, value in ESM_embs_dict.items():\n",
    "    \n",
    "    #print(f\"Working with {count}/{len(ESM_embs_dict)}\", end = \"\\r\")\n",
    "    \n",
    "    #if structure embeddings exist - use it , else use zeros\n",
    "    esm = value\n",
    "    ps = PS_embs_dict[key]\n",
    "\n",
    "    #Sanity check dimensions\n",
    "    assert esm.shape == ps.shape\n",
    "        \n",
    "    #Concatenate the embeddings and add to dict\n",
    "    Xs = torch.cat((esm,ps),1)\n",
    "    cat_embs_dict[key] = Xs\n",
    "\n",
    "print(f\"Concatenated embeddings from {len(cat_embs_dict)} proteins\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e858a55",
   "metadata": {},
   "source": [
    "## Check distribution of data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c1a8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#NESG labels\n",
    "nesg_counts = Counter(NESG_label)\n",
    "df_label1 = pd.DataFrame.from_dict(nesg_counts, orient='index')\n",
    "plt1 = df_label1.plot(kind='bar')\n",
    "plt1.legend([\"NESG\"])\n",
    "\n",
    "#Psi_bio labels\n",
    "psi_bio_counts = Counter(psi_bio_label)\n",
    "df_label2 = pd.DataFrame.from_dict(psi_bio_counts, orient='index')\n",
    "plt2 = df_label2.plot(kind='bar')\n",
    "plt2.legend([\"Psi-Bio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d30bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that calculates amino acid distribution\n",
    "def aa_dist(seq):\n",
    "    counter = Counter(seq)\n",
    "    aas = [\"A\",\"R\",\"N\",\"D\",\"B\",\"C\",\"Q\",\"E\",\"G\",\"H\",\"I\",\"L\",\"K\",\"M\",\"F\",\"P\",\"S\",\"T\",\"W\",\"V\"]\n",
    "    dist = []\n",
    "    for aa in aas:\n",
    "        if aa in counter:\n",
    "            dist.append(counter[aa]/len(seq))\n",
    "        else:\n",
    "            dist.append(0)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf75e21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Ensure proper label/emb/clust order\n",
    "data_labels = []\n",
    "embs_X = []\n",
    "clusters = []\n",
    "count = 0\n",
    "total = len(cat_embs_dict)\n",
    "for key,embs in cat_embs_dict.items():\n",
    "    count += 1\n",
    "    print(f\"working with {count}/{total}\", end = \"\\r\")\n",
    "    row_num = df.loc[df['ID'] == key]\n",
    "    label = [row_num.norm_NESG_label.item(),int(row_num.PSI_BIO_label)]\n",
    "    \n",
    "    #Also, add in the extra info\n",
    "    template = [0] * len(embs)\n",
    "    extra = aa_dist(row_num.sequence.item())\n",
    "    extra_inf = extra + [len(row_num.sequence.item())]\n",
    "    template = [extra_inf for x in template]\n",
    "    extra_inf = torch.FloatTensor(template)\n",
    "    \n",
    "    #Append all in correct order\n",
    "    clusters.append(row_num.cluster)\n",
    "    data_labels.append(label)\n",
    "    embs_X.append(torch.cat((embs,extra_inf), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c2ae6",
   "metadata": {},
   "source": [
    "### Create DataSet and DataLoader functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d021988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset function\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return (self.X[idx], torch.tensor(self.y[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c9c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create collate function for padding sequences\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0) \n",
    "    return xx_pad, yy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e86f878",
   "metadata": {},
   "source": [
    "# Make functions for easier model assesment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabdf451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make model for saving models\n",
    "def save_model(filepath, n_epochs, model_conv):\n",
    "    #Save the trained model in various ways to ensure no loss of model\n",
    "    \n",
    "    #Create the folder\n",
    "    isExist = os.path.exists(filepath)\n",
    "    if not isExist:\n",
    "        os.makedirs(filepath)\n",
    "\n",
    "    ### METHOD 1 ###\n",
    "    torch.save(model_conv.state_dict(), filepath+\"/model_conv.state_dict\")\n",
    "\n",
    "    #Later to restore:\n",
    "    #model.load_state_dict(torch.load(filepath))\n",
    "    #model.eval()\n",
    "\n",
    "    ### METHOD 2 ###\n",
    "    state = {\n",
    "        'epoch': n_epochs,\n",
    "        'state_dict': model_conv.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(state, filepath+\"/model_conv.state\")\n",
    "\n",
    "    #Later to restore:\n",
    "    #model.load_state_dict(state['state_dict'])\n",
    "    #optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "\n",
    "    ### METHOD 3 ###\n",
    "    torch.save(model_conv, filepath+\"/model_conv.full\")\n",
    "\n",
    "    #Later to restore:\n",
    "    #model = torch.load(filepath)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd390c",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64f9b38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "input_size = 60\n",
    "hidden_size = 64\n",
    "num_layers = 3\n",
    "num_classes_nesg = 6 #7\n",
    "num_classes_psibio = 2 #3\n",
    "batch_size = 96 # Is defined in the data loader\n",
    "n_epochs = 8 #51\n",
    "lr = 0.001  #0.01\n",
    "dropout = 0.4\n",
    "weight_decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce5c9fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Define Bi_LSTM model\n",
    "\n",
    "class Bi_LSTM(nn.Module) :\n",
    "    def __init__(self, input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, num_classes_nesg = num_classes_nesg, num_classes_psibio = num_classes_psibio, dropout = dropout) :\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes_nesg = num_classes_nesg\n",
    "        self.num_classes_psibio = num_classes_psibio\n",
    "        self.dropout = dropout\n",
    "            \n",
    "        #Initialize the LSTM layer \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional = True, batch_first=True, dropout = dropout)\n",
    "        \n",
    "        #Initialize ReLU layer\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #Initilize the linear layers for nesg labels \n",
    "        self.linear1 = nn.Linear((hidden_size * 2)+21, 1)\n",
    "        \n",
    "        #Initilize the linear layers for psibio labels\n",
    "        self.linear2 = nn.Linear((hidden_size * 2)+21, num_classes_psibio)\n",
    "        \n",
    "        #Initialize softmax activation function \n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        #Initialize the hidden state with random numbers\n",
    "        self.hidden = (torch.randn(1, 1, self.hidden_size),torch.randn(1, 1, self.hidden_size))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Split embeddings and extra info for last dense layer\n",
    "        embs, extra = torch.split(x, [60,21], dim=2)\n",
    "        extra = torch.squeeze(extra)\n",
    "        extra = extra.mean(1)\n",
    "        #print(f\"extra shape: {extra.shape}\")\n",
    "        #extra shape: torch.Size([128, 21])\n",
    "        \n",
    "        #batch normalize data\n",
    "        self.bnorm = nn.BatchNorm1d(num_features=embs.shape[1])\n",
    "        norm_data = self.bnorm(embs)\n",
    "        \n",
    "        #Initialize the hidden states and cell states\n",
    "        h_0 = torch.autograd.Variable(torch.zeros(self.num_layers*2, norm_data.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = torch.autograd.Variable(torch.zeros(self.num_layers*2, norm_data.size(0), self.hidden_size)) #internal state\n",
    "    \n",
    "        #forward through the lstm layer\n",
    "        #print(f\"initial shape: {norm_data.shape}\")\n",
    "        #initial shape: torch.Size([128, 804, 60])\n",
    "        lstm_out,(ht, ct) = self.lstm(norm_data,(h_0, c_0))\n",
    "        \n",
    "        \n",
    "        #concatenate states from both directions\n",
    "        lstm_ht = torch.cat([ht[-1,:, :], ht[-2,:,:]], dim=1)\n",
    "        #print(f\"after lstm shape: {lstm_ht.shape}\")\n",
    "        #after lstm shape: torch.Size([128, 128])\n",
    "        \n",
    "        #Add the extra information before going through last dense layers\n",
    "        collect = torch.cat((lstm_ht, extra), dim=1)\n",
    "        \n",
    "        #forward through relu layer\n",
    "        #print(f\"with_collection shape: {collect.shape}\")\n",
    "        #with_collection shape: torch.Size([128, 149])\n",
    "        relu_nesg = self.relu(collect)\n",
    "        relu_psibio = self.relu(collect)\n",
    "        \n",
    "        #forward through linear layer 1\n",
    "        nesg_linear = self.linear1(relu_nesg)\n",
    "        psibio_linear = self.linear2(relu_psibio)\n",
    "        \n",
    "        #Add sigmoid activation function\n",
    "        sigmoid_nesg = self.sigmoid(nesg_linear)\n",
    "        \n",
    "        #Define output\n",
    "        out1 = sigmoid_nesg\n",
    "        out2 = psibio_linear\n",
    "\n",
    "        return [out1, out2]\n",
    "\n",
    "#Define the model, optimizer and loss function (removed  weight_decay = weight_decay) (removed weight=class_weights_nesg,)\n",
    "model = Bi_LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, num_classes_nesg = num_classes_nesg, num_classes_psibio = num_classes_psibio, dropout = dropout)\n",
    "model.to(device)\n",
    "loss_nesg = nn.MSELoss(reduction='none')\n",
    "loss_psibio = nn.CrossEntropyLoss(ignore_index=9, reduction = \"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e605e993",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1a8a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Train model function\n",
    "def train_model(model, loss_func1, loss_func2, optimizer, n_epochs):\n",
    "    \"\"\"Return trained model\"\"\"\n",
    "    \n",
    "    #Train network\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        \n",
    "        #Iterate through batches\n",
    "        for i, (embs, labels) in enumerate(train_loader):\n",
    "            \n",
    "            #reset optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Print sceen output\n",
    "            str_epoch = format(epoch, '03d')\n",
    "            str_batch = format(i+1, '03d')\n",
    "            print(f\"Epoch: {str_epoch}, batch: {str_batch}\", end=\"\\r\")     \n",
    "\n",
    "            #Format labels \n",
    "            nesg_labels = torch.tensor([label[0] for label in labels])\n",
    "            psibio_labels = torch.tensor([int(label[1]) for label in labels], dtype = torch.long)           \n",
    "            \n",
    "            #Predict labels (forward)\n",
    "            y_pred = model(embs)\n",
    "            y_pred1 = torch.squeeze(y_pred[0])\n",
    "            y_pred2 = y_pred[1]   \n",
    "            \n",
    "            #Make a mask vector\n",
    "            multiply = torch.tensor([0 if x == 6 else 1 for x in nesg_labels])\n",
    "\n",
    "            #Calculate MSE loss using masking\n",
    "            loss1 = loss_func1(y_pred1, nesg_labels)\n",
    "            non_zero_elements = multiply.sum()\n",
    "            masked_loss = (loss1*multiply).sum()/non_zero_elements \n",
    "            \n",
    "            #Calculate Cross Entropy loss\n",
    "            loss2 = loss_func2(y_pred2, psibio_labels)\n",
    "            \n",
    "            #Combine loss (backward)\n",
    "            combined_loss = masked_loss + loss2*0.25\n",
    "            combined_loss.backward()\n",
    "            \n",
    "            #optimize\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        #Save model for each epoch\n",
    "        filepath = f\"./model/model_{epoch}\"\n",
    "        save_model(filepath, epoch, model)\n",
    "        \n",
    "        \n",
    "    #Return model, loss values and MCC\n",
    "    return model\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea063040",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare data loading and model \n",
    "train = ProteinDataset(embs_X,data_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, collate_fn=pad_collate, shuffle = True)\n",
    "model = Bi_LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, num_classes_nesg = num_classes_nesg, num_classes_psibio = num_classes_psibio, dropout = dropout)\n",
    "loss_nesg = nn.MSELoss(reduction='none')\n",
    "loss_psibio = nn.CrossEntropyLoss(ignore_index=9, reduction = \"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "\n",
    "#Train the model\n",
    "model = train_model(model,loss_nesg, loss_psibio, optimizer, n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa7afc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a00db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3714177a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e581f83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cfb677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c6ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2fc024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbf4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e65677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26623b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
