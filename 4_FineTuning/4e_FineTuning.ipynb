{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62cc0946",
   "metadata": {},
   "source": [
    "# Finetune a multitask model on concatenated embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113edd3",
   "metadata": {},
   "source": [
    "This notebook aims at concatenating the ESM and PS embeddings and perform multitask learning in order to learn solubility patterns through multitask learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d02fc",
   "metadata": {},
   "source": [
    "### Import and initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21595bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Import stuff\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import seaborn as sn\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import linregress\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60127e3",
   "metadata": {},
   "source": [
    "### Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd63ff7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Only use cpu\n",
    "device = \"cpu\"\n",
    "\n",
    "#Assign embedding folder\n",
    "ESM_EMB_PATH = \"./PCA_reduced/ESM_embeddings/\"\n",
    "PS_EMB_PATH = \"./PCA_reduced/PS_embeddings/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17dc1de",
   "metadata": {},
   "source": [
    "## Load experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5188d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load experimental values\n",
    "exp = pd.read_csv(\"jain_full.csv\", sep=\";\")\n",
    "exp.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186fc326",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4212ff03",
   "metadata": {},
   "source": [
    "## Load clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e6474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#min seq ID for clustering: 0.65\n",
    "clusters = pd.read_csv(\"./ABDB_clu_80.tsv\", sep=\"\\t\",  header=None)\n",
    "clusters= clusters.rename(columns={0: 'rep', 1 :'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848260f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a cluster dictionary\n",
    "cluster_temp_dict = {}\n",
    "cluster_dict = {}\n",
    "count = 0\n",
    "for i, row in clusters.iterrows():\n",
    "    if row[\"rep\"] in cluster_temp_dict:\n",
    "        cluster_dict[row[\"id\"]] = cluster_temp_dict[row[\"rep\"]]\n",
    "    else:\n",
    "        cluster_temp_dict[row[\"rep\"]] = count \n",
    "        count += 1\n",
    "        cluster_dict[row[\"id\"]] = cluster_temp_dict[row[\"rep\"]]\n",
    "        \n",
    "print(f\"Total amount of clusters: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50233d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append cluster info to df\n",
    "clusters = []\n",
    "for i, row in exp.iterrows():\n",
    "    name = row[\"Name\"]\n",
    "    clusters.append(cluster_dict[name])\n",
    "exp[\"cluster\"] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c621f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7754e9a7",
   "metadata": {},
   "source": [
    "## Check label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad3c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot histogram\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "_ = plt.hist(exp[\"AC-SINS\"], bins = 40)\n",
    "_ = plt.title(\"norm AC-SINS values distribution\", fontsize=20)\n",
    "_ = plt.xticks(np.arange(-1, 31, step=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02efa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize AC-SINS\n",
    "norm_ac = [(data - min(exp[\"AC-SINS\"])) / (max(exp[\"AC-SINS\"]) - min(exp[\"AC-SINS\"])) for data in exp[\"AC-SINS\"]]\n",
    "exp[\"norm_AC-SINS\"] = norm_ac\n",
    "\n",
    "#Normalize HIC\n",
    "norm_hic = [(data - min(exp[\"HIC\"])) / (max(exp[\"HIC\"]) - min(exp[\"HIC\"])) for data in exp[\"HIC\"]]\n",
    "exp[\"norm_HIC\"] = norm_hic\n",
    "\n",
    "#Add fake labels for testing\n",
    "rng = np.random.default_rng(12345)\n",
    "rand = rng.random(len(norm_ac))\n",
    "exp[\"fake\"] = rand\n",
    "\n",
    "#Binary classifictaion\n",
    "bc = [0 if val <= 5 else 1 for val in exp[\"AC-SINS\"]]\n",
    "exp[\"BC\"] = bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85facd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bffc588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make label dict\n",
    "label_dict = {}\n",
    "for i, row in exp.iterrows():\n",
    "    label_dict[row[\"Name\"]] = [row[\"BC\"], row[\"Name\"]]\n",
    "    \n",
    "print(len(label_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56937df0",
   "metadata": {},
   "source": [
    "### Load ESM embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db226084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and format embeddings in a dict\n",
    "ESM_embs_dict = dict()     \n",
    "for file in os.listdir(ESM_EMB_PATH):\n",
    "    name = file.split(\".\")[0].split(\"_\")[-1]\n",
    "    if file.endswith(\".pt\"):\n",
    "        print (f\"working with file: {file}\", end=\"\\r\")\n",
    "        tensor_in = torch.load(f'{ESM_EMB_PATH}/{file}')\n",
    "        ESM_embs_dict[name] = tensor_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afda86ee",
   "metadata": {},
   "source": [
    "### Load PS embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99e9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and format embeddings\n",
    "PS_embs_dict = dict()\n",
    "for file in os.listdir(PS_EMB_PATH):\n",
    "    name = file.split(\".\")[0].split(\"_\")[-1]\n",
    "    if file.endswith(\".pt\"):\n",
    "        print (f\"working with file: {file}\", end=\"\\r\")\n",
    "        tensor_in = torch.load(f'{PS_EMB_PATH}/{file}')\n",
    "        PS_embs_dict[name] = tensor_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d0a07",
   "metadata": {},
   "source": [
    "### Concatenate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate PS and ESm embeddings\n",
    "cat_embs_dict = dict()\n",
    "count = 0\n",
    "\n",
    "# Iterate through sequence embeddings\n",
    "for key, value in ESM_embs_dict.items():\n",
    "    count += 1\n",
    "    print(f\"Working with {count}/{len(ESM_embs_dict)}\", end = \"\\r\")\n",
    "    \n",
    "    #if structure embeddings exist - use it , else use zeros\n",
    "    esm = value\n",
    "    ps = PS_embs_dict[key]\n",
    "\n",
    "    #Sanity check dimensions\n",
    "    assert esm.shape == ps.shape\n",
    "        \n",
    "    #Concatenate the embeddings and add to dict\n",
    "    Xs = torch.cat((esm,ps),1)\n",
    "    cat_embs_dict[key] = Xs\n",
    "\n",
    "print(f\"Concatenated embeddings from {len(cat_embs_dict)} proteins\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6835616e",
   "metadata": {},
   "source": [
    "### Load sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0388d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get sequences from fasta file\n",
    "fastas = {}\n",
    "with open(\"./antibody_bulk.fsa\", \"r\") as fasta:\n",
    "    for line in fasta:\n",
    "        if line.startswith(\">\"):\n",
    "            header = line.strip()[1:]\n",
    "        else:\n",
    "            seq = line.strip()\n",
    "            fastas[header] = seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f047bb47",
   "metadata": {},
   "source": [
    "### Prepare data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83101d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that calculates amino acid distribution\n",
    "def aa_dist(seq):\n",
    "    counter = Counter(seq)\n",
    "    aas = [\"A\",\"R\",\"N\",\"D\",\"B\",\"C\",\"Q\",\"E\",\"G\",\"H\",\"I\",\"L\",\"K\",\"M\",\"F\",\"P\",\"S\",\"T\",\"W\",\"V\"]\n",
    "    dist = []\n",
    "    for aa in aas:\n",
    "        if aa in counter:\n",
    "            dist.append(counter[aa]/len(seq))\n",
    "        else:\n",
    "            dist.append(0)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7043d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure dimensions/info in embeddings\n",
    "embs_X = []\n",
    "data_names = []\n",
    "clusters = []\n",
    "data_labels = []\n",
    "\n",
    "for key,embs in cat_embs_dict.items():\n",
    "    row_num = exp.loc[exp['Name'] == key]\n",
    "\n",
    "    #Also, add in the extra info\n",
    "    template = [0] * len(embs)\n",
    "    extra = aa_dist(fastas[key])\n",
    "    extra_inf = extra + [len(fastas[key])]\n",
    "    template = [extra_inf for x in template]\n",
    "    extra_inf = torch.FloatTensor(template)\n",
    "    \n",
    "    #Get proper labels\n",
    "    if key in list(exp[\"Name\"]):\n",
    "        data_labels.append(label_dict[key][0])\n",
    "        data_names.append(label_dict[key][1])\n",
    "    \n",
    "        #Append all\n",
    "        embs_X.append(torch.cat((embs,extra_inf), 1))\n",
    "        clusters.append(row_num.cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embs_X[50].shape)\n",
    "print(len(embs_X))\n",
    "print(len(data_labels))\n",
    "print(len(clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c43654",
   "metadata": {},
   "source": [
    "### Dataset, data split and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define my own group-K-fold splitter\n",
    "def groupkfold(data_X, data_y, data_cluster, n=10):\n",
    "    \"\"\"Split the data for cross fold validation\"\"\"\n",
    "    \n",
    "    #initialize\n",
    "    folds = []\n",
    "    partitions = {}\n",
    "    total_size = len(data_cluster)\n",
    "    part_size = total_size/(n+1)\n",
    "    cluster_dict = {}\n",
    "    \n",
    "    #unique clusters\n",
    "    u_clust = set(data_cluster)\n",
    "    count = 0\n",
    "    for cluster in u_clust:\n",
    "        count += 1\n",
    "        temp = []\n",
    "        for i, x in enumerate(data_cluster):\n",
    "            if x == cluster:\n",
    "                temp.append(i)\n",
    "        cluster_dict[count] = temp\n",
    "        \n",
    "    #Sort clusters by size \n",
    "    s_clust = sorted(cluster_dict.items(), key=lambda x: len(x[1]),reverse=True)\n",
    "    \n",
    "    #Split into partitions\n",
    "    counter = 0\n",
    "    skips = []\n",
    "    for cl, part in s_clust:\n",
    "\n",
    "        counter += 1\n",
    "        if counter%(n+2) == 0:\n",
    "            counter = 1\n",
    "            \n",
    "        while counter in skips:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter in partitions:\n",
    "            if counter not in skips:\n",
    "                partitions[counter] += part\n",
    "                \n",
    "                if len(partitions[counter]) > part_size:\n",
    "                    skips.append(counter)\n",
    "                \n",
    "            else:\n",
    "                print(\"Something went wrong\")\n",
    "                sys.exit(1)\n",
    "        else:\n",
    "            partitions[counter] = part\n",
    "\n",
    "        \n",
    "    #Double check that all is good\n",
    "    tester_size = 0\n",
    "    tester_sizes = []\n",
    "    for p, part in partitions.items():\n",
    "        tester_size += len(part)\n",
    "        tester_sizes.append(len(part))\n",
    "    if tester_size != total_size:\n",
    "        print(\"not all data is included. Aborting.\")\n",
    "        print(tester_size, total_size)\n",
    "        sys.exit(1)\n",
    "    elif (max(tester_sizes)-min(tester_sizes)) > 10:\n",
    "        print(\"Partitions does not match\")\n",
    "        print(f\"Max: {max(tester_sizes)}, Min: {min(tester_sizes)}\")\n",
    "        sys.exit(1)\n",
    "    elif len(partitions) != (n+1):\n",
    "        print(\"The true length and needed lenght are not identical\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    #Make the folds\n",
    "    ps = []\n",
    "    partitions = sorted(partitions.items(), key=lambda x: x[0])\n",
    "    \n",
    "    #First get the test partition\n",
    "    test_idx = partitions[round(n/2)][1]\n",
    "    del partitions[(round(n/2))]\n",
    "    for p, part in enumerate(partitions):\n",
    "        ps.append(p)\n",
    "        val_idx = partitions[p][1]\n",
    "        train_idx = [value[1] for i,value in enumerate(partitions) if i not in [p]]\n",
    "        train_idx = [x for sublist in train_idx for x in sublist]\n",
    "        folds.append([train_idx,val_idx,test_idx])\n",
    "        \n",
    "    return folds\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca7806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset function\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return (self.X[idx], torch.tensor(self.y[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create collate function for padding sequences\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0) \n",
    "    return xx_pad, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b3e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make model for saving models\n",
    "def save_model(filepath, epoch, model, train_loss_values, train_r, train_p, train_AUC, train_MCC, train_labels,train_pred,\n",
    "               val_loss_values, val_r, val_p, val_AUC, val_MCC, val_labels,val_pred,\n",
    "               test_loss_values, test_r, test_p, test_AUC, test_MCC, test_labels,test_pred):\n",
    "    \n",
    "    #Save the trained model in various ways to ensure no loss of model\n",
    "    \n",
    "    #Create the folder\n",
    "    isExist = os.path.exists(filepath)\n",
    "    if not isExist:\n",
    "        os.makedirs(filepath)\n",
    "\n",
    "    ### METHOD 1 ###\n",
    "    torch.save(model.state_dict(), filepath+\"/model_conv.state_dict\")\n",
    "\n",
    "    #Later to restore:\n",
    "    #model.load_state_dict(torch.load(filepath))\n",
    "    #model.eval()\n",
    "\n",
    "    ### METHOD 2 ###\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'train_loss' : train_loss_values,\n",
    "        'val_loss' : val_loss_values,\n",
    "        'test_loss' : test_loss_values,\n",
    "        'train_r': train_r,\n",
    "        'train_p': train_p,\n",
    "        'val_r': val_r,\n",
    "        'val_p': val_p,\n",
    "        'test_r': test_r,\n",
    "        'test_p': test_p,\n",
    "        'train_AUC': train_AUC,\n",
    "        'val_AUC': val_AUC,\n",
    "        'test_AUC': test_AUC,\n",
    "        'train_MCC': train_MCC,\n",
    "        'val_MCC': val_MCC,\n",
    "        'test_MCC': test_MCC,\n",
    "        'train_labels':train_labels,\n",
    "        'val_labels': val_labels,\n",
    "        'test_labels':test_labels,\n",
    "        'train_pred': train_pred, \n",
    "        'val_pred': val_pred,\n",
    "        'test_pred': test_pred,\n",
    "    }\n",
    "\n",
    "    torch.save(state, filepath+\"/model_conv.state\")\n",
    "\n",
    "    #Later to restore:\n",
    "    #model.load_state_dict(state['state_dict'])\n",
    "    #optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "\n",
    "    ### METHOD 3 ###\n",
    "    torch.save(model, filepath+\"/model_conv.full\")\n",
    "\n",
    "    #Later to restore:\n",
    "    #model = torch.load(filepath)\n",
    "    \n",
    "# Make model for saving models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f7d51",
   "metadata": {},
   "source": [
    "## START FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ff601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits for testing\n",
    "test_embs = embs_X\n",
    "test_label = data_labels\n",
    "test_clusters = [intg.tolist()[0] for intg in clusters]\n",
    "\n",
    "folds = groupkfold(test_embs, test_label, test_clusters, n=10)\n",
    "foldperf = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9c0ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate/test model\n",
    "def test_model(model, optimizer, data_loader, loss_fn):\n",
    "    \"Run model in evaluation mode on a dataset\"\n",
    "    val_running_loss = 0.0\n",
    "    val_pred = []\n",
    "    val_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (embs,labels) in enumerate(data_loader):\n",
    "            labels = torch.tensor(labels)\n",
    "            labels = labels.float()\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(embs)\n",
    "            y_pred = y_pred.squeeze()\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            val_running_loss += loss.item() * embs.size(0)\n",
    "            acsins_pred = [pred.item() for pred in y_pred]\n",
    "            val_pred.append(acsins_pred)\n",
    "            val_labels.append(labels)\n",
    "    model.train() \n",
    "    return val_pred, val_labels, val_running_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62de048",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "input_size = 60\n",
    "hidden_size = 64\n",
    "num_layers = 3\n",
    "num_classes_nesg = 6 #7\n",
    "num_classes_psibio = 2 #3\n",
    "batch_size = 17\n",
    "n_epochs = 100 \n",
    "lr = 0.0001 #0.0001\n",
    "dropout = 0.4\n",
    "weight_decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05476362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Bi_LSTM model\n",
    "class Bi_LSTM(nn.Module) :\n",
    "    def __init__(self, input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, num_classes_nesg = num_classes_nesg, num_classes_psibio = num_classes_psibio, dropout = dropout) :\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes_nesg = num_classes_nesg\n",
    "        self.num_classes_psibio = num_classes_psibio\n",
    "        self.dropout = dropout\n",
    "            \n",
    "        #Initialize the LSTM layer \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional = True, batch_first=True)\n",
    "        \n",
    "        #Initialize ReLU layer\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        ## FOR FINETUNING; SKIP THE LINEAR LAYER (no need to lower complexity yet) ##\n",
    "        \n",
    "        #Initilize the linear layers for nesg labels \n",
    "        self.linear1 = nn.Linear((hidden_size * 2)+21, 1)\n",
    "        \n",
    "        #Initilize the linear layers for psibio labels\n",
    "        self.linear2 = nn.Linear((hidden_size * 2)+21, num_classes_psibio)\n",
    "        \n",
    "        #Initialize sigmoid activation function \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        ## THIS IS THE NEW LAST LINEAR LAYERS ###\n",
    "        #Initialize last layer\n",
    "        self.last1 = nn.Linear(((hidden_size * 2)+21), 32)\n",
    "        self.last2 = nn.Linear(32,1)\n",
    "        self.last_activation = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Split embeddings and extra info for last dense layer\n",
    "        embs, extra = torch.split(x, [60,21], dim=2)\n",
    "        extra = torch.squeeze(extra)\n",
    "        extra = extra.mean(1)\n",
    "        #print(f\"extra shape: {extra.shape}\")\n",
    "        #extra shape: torch.Size([128, 21])\n",
    "        \n",
    "        #batch normalize data\n",
    "        self.bnorm = nn.BatchNorm1d(num_features=embs.shape[1])\n",
    "        norm_data = self.bnorm(embs)\n",
    "        \n",
    "        #forward through the lstm layer\n",
    "        #print(f\"initial shape: {norm_data.shape}\")\n",
    "        #initial shape: torch.Size([128, 804, 60])\n",
    "        lstm_out,(ht, ct) = self.lstm(norm_data)\n",
    "        \n",
    "        \n",
    "        #concatenate states from both directions\n",
    "        lstm_ht = torch.cat([ht[-1,:, :], ht[-2,:,:]], dim=1)\n",
    "        #print(f\"after lstm shape: {lstm_ht.shape}\")\n",
    "        #after lstm shape: torch.Size([128, 128])\n",
    "        \n",
    "        #Add the extra information before going through last dense layers\n",
    "        collect = torch.cat((lstm_ht, extra), dim=1)\n",
    "\n",
    "        #forward through relu layer\n",
    "        #print(f\"with_collection shape: {collect.shape}\")\n",
    "        #with_collection shape: torch.Size([128, 149])\n",
    "        relu_nesg = self.relu(collect)\n",
    "        relu_psibio = self.relu(collect)\n",
    "        \n",
    "        #Add last layer \n",
    "        #print(f\"with_collection shape: {collect.shape}\")\n",
    "        last_out1 = self.last1(collect)\n",
    "        last_out2 = self.last2(last_out1)\n",
    "        out = self.last_activation(last_out2)\n",
    "\n",
    "\n",
    "        return out\n",
    "\n",
    "#Define the model, optimizer and loss function (removed  weight_decay = weight_decay) (removed weight=class_weights_nesg,)\n",
    "model = Bi_LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, num_classes_nesg = num_classes_nesg, num_classes_psibio = num_classes_psibio, dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aa5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "state = torch.load(\"../3_PreTraining/model/model_8/model_conv.state\")\n",
    "model.load_state_dict(torch.load(\"../3_PreTraining/model/model_8/model_conv.state_dict\"), strict=False)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d6061",
   "metadata": {},
   "source": [
    "## Manually check out that the model is ready to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17986c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state[\"state_dict\"]['lstm.weight_hh_l0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b999ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = 0\n",
    "catch = 0\n",
    "for param in model.parameters():\n",
    "    full += 1\n",
    "    if param.requires_grad:\n",
    "        catch += 1\n",
    "print(f\"{catch}/{full} parameters have gradients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model function\n",
    "def train_model(model, loss_func1, optimizer, n_epochs):\n",
    "    \"\"\"Return trained model\"\"\"\n",
    "    \n",
    "    #Early stopping \n",
    "    patience = 10 #pre-training has a patience of 3\n",
    "    triggers = 0\n",
    "    total_triggers = 0\n",
    "    min_val_loss = 1000\n",
    "    \n",
    "    #Keep track of loss\n",
    "    train_loss_values = []\n",
    "    val_loss_values = []\n",
    "    test_loss_values = []\n",
    "    \n",
    "    #Keep track of AUC\n",
    "    train_AUC = []\n",
    "    val_AUC = []\n",
    "    test_AUC = []\n",
    "    \n",
    "    #Keep track of MCC\n",
    "    train_MCC = []\n",
    "    val_MCC = []\n",
    "    test_MCC = []\n",
    "    \n",
    "    #Keep track of r and p values\n",
    "    train_r = []\n",
    "    train_p = []\n",
    "    val_r = []\n",
    "    val_p = []\n",
    "    test_r = []\n",
    "    test_p = []\n",
    "    \n",
    "    #Test if weights are updated\n",
    "    w0 = []\n",
    "    w1 = []\n",
    "    w2 = []\n",
    "    w3 = []\n",
    "    w4 = []\n",
    "    w5 = []\n",
    "    w6 = []\n",
    "    w7 = []\n",
    "    w8 = []\n",
    "    w9 = []\n",
    "    w10 = []\n",
    "    w11 = []\n",
    "    w12 = []\n",
    "    w13 = []\n",
    "    \n",
    "    #Train network\n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        \n",
    "        #Keep track of train loss\n",
    "        train_running_loss = 0.0\n",
    "        train_pred = []\n",
    "        train_labels = []\n",
    "        \n",
    "        #Keep track of val loss\n",
    "        val_running_loss = 0.0\n",
    "        val_pred = []\n",
    "        val_labels = []\n",
    "        \n",
    "        #Chekck if the weights are updated\n",
    "        params = model.state_dict()\n",
    "        w0.append(params[\"last1.weight\"].clone())\n",
    "        w1.append(params[\"last2.weight\"].clone())\n",
    "        w2.append(params['lstm.weight_ih_l0'].clone())\n",
    "        w3.append(params['lstm.weight_hh_l0'].clone())\n",
    "        w4.append(params['lstm.weight_ih_l0_reverse'].clone())\n",
    "        w5.append(params['lstm.weight_hh_l0_reverse'].clone())\n",
    "        w6.append(params['lstm.weight_ih_l1'].clone())\n",
    "        w7.append(params['lstm.weight_hh_l1'].clone())\n",
    "        w8.append(params['lstm.weight_ih_l1_reverse'].clone())\n",
    "        w9.append(params['lstm.weight_hh_l1_reverse'].clone())\n",
    "        w10.append(params['lstm.weight_ih_l2'].clone())\n",
    "        w11.append(params['lstm.weight_hh_l2'].clone())\n",
    "        w12.append(params['lstm.weight_ih_l2_reverse'].clone())\n",
    "        w13.append(params['lstm.weight_hh_l2_reverse'].clone())\n",
    "        print(f\"Epoch: {epoch}\", end=\"\\n\")\n",
    "        \n",
    "        \n",
    "        #Iterate through batches\n",
    "        for i, (embs, labels) in enumerate(train_loader):\n",
    "            labels = torch.tensor(labels)\n",
    "            labels = labels.float()\n",
    "            \n",
    "            #reset optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Print sceen output\n",
    "            str_epoch = format(epoch, '03d')\n",
    "            #print(f\"Epoch: {str_epoch} batch: {i}\", end=\"\\n\")     \n",
    " \n",
    "            #Predict labels (forward)\n",
    "            y_pred = model(embs)\n",
    "            y_pred = y_pred.squeeze()\n",
    "\n",
    "            #Calculate MSE loss \n",
    "            loss = loss_func1(y_pred, labels)\n",
    "            loss.backward()\n",
    "            train_running_loss += loss.item() * embs.size(0)\n",
    "            \n",
    "            #optimize\n",
    "            optimizer.step()\n",
    "            \n",
    "            #collect prediction and labels for comparison (overwrites every epoch)\n",
    "            acsins = [pred.item() for pred in y_pred]\n",
    "            train_pred.append(acsins)\n",
    "            train_labels.append(labels)\n",
    "            \n",
    "        #Run model in evaluation mode on validation set\n",
    "        val_pred, val_labels, val_running_loss = test_model(model, optimizer, val_loader, loss_fn)\n",
    "        \n",
    "        #Run model in evaluation mode on test set\n",
    "        test_pred, test_labels, test_running_loss = test_model(model, optimizer, test_loader, loss_fn)\n",
    "          \n",
    "        #Collect loss after each epoch\n",
    "        train_loss_values.append(train_running_loss / len(train_X))\n",
    "        val_loss_values.append(val_running_loss / len(val_X))\n",
    "        test_loss_values.append(test_running_loss/len(test_X))\n",
    "        \n",
    "        #Format predictions \n",
    "        train_pred = [item for sublist in train_pred for item in sublist]\n",
    "        train_labels = [item for sublist in train_labels for item in sublist]\n",
    "        val_pred = [item for sublist in val_pred for item in sublist]\n",
    "        val_labels = [item for sublist in val_labels for item in sublist]\n",
    "        test_pred = [item for sublist in test_pred for item in sublist]\n",
    "        test_labels = [item for sublist in test_labels for item in sublist]\n",
    "        \n",
    "        #Collect spearman corelation\n",
    "        #train_correlation, p_value = spearmanr(train_labels, train_pred)\n",
    "        #train_SCC.append(train_correlation)\n",
    "        #val_correlation, p_value = spearmanr(val_labels, val_pred)\n",
    "        #val_SCC.append(val_correlation)\n",
    "        #test_correlation, p_value = spearmanr(test_labels, test_pred)\n",
    "        #test_SCC.append(test_correlation)\n",
    "        \n",
    "        #Collect AUC\n",
    "        auc_train = roc_auc_score(train_labels,train_pred)\n",
    "        auc_val = roc_auc_score(val_labels,val_pred)\n",
    "        auc_test = roc_auc_score(test_labels,test_pred)\n",
    "        train_AUC.append(auc_train)\n",
    "        val_AUC.append(auc_val)\n",
    "        test_AUC.append(auc_test)\n",
    "        \n",
    "        #Collect pearson corelation\n",
    "        #train_correlation, p_value = pearsonr(train_labels, train_pred)\n",
    "        #train_PCC.append(train_correlation)\n",
    "        #val_correlation, p_value = pearsonr(val_labels, val_pred)\n",
    "        #val_PCC.append(val_correlation)\n",
    "        \n",
    "        #Collect MCC\n",
    "        train_pred_round = [0 if x <= 0.35 else 1 for x in train_pred]\n",
    "        val_pred_round = [0 if x <= 0.35 else 1 for x in val_pred]\n",
    "        test_pred_round = [round(x) for x in test_pred]\n",
    "        train_MCC.append(matthews_corrcoef(train_labels, train_pred_round))\n",
    "        val_MCC.append(matthews_corrcoef(val_labels, val_pred_round))\n",
    "        test_MCC.append(matthews_corrcoef(test_labels, test_pred_round))\n",
    "        \n",
    "        #Collect R^2 and p-value\n",
    "        train_r.append(accuracy_score(train_labels,train_pred_round))\n",
    "        val_r.append(accuracy_score(val_labels,val_pred_round))\n",
    "        test_r.append(accuracy_score(test_labels,test_pred_round))\n",
    "        \n",
    "        slope, intercept, train_r_value, train_p_value, std_err = linregress(train_labels, train_pred)\n",
    "        train_p.append(train_p_value)\n",
    "        slope, intercept, val_r_value, val_p_value, std_err = linregress(val_labels, val_pred)\n",
    "        val_p.append(val_p_value)\n",
    "        slope, intercept, test_r_value, test_p_value, std_err = linregress(test_labels, test_pred)\n",
    "        test_p.append(test_p_value)\n",
    "            \n",
    "        if epoch%1 == 0:\n",
    "            filepath = f\"./models/fold{fold}/finetune_{epoch}_epochs\"\n",
    "            save_model(filepath,  epoch, model, train_loss_values, train_r, train_p, train_AUC, train_MCC, train_labels,train_pred, val_loss_values, val_r, val_p, val_AUC, val_MCC, val_labels,val_pred,test_loss_values, test_r, test_p, test_AUC, test_MCC, test_labels,test_pred)            \n",
    "            \n",
    "            print(f\"triggers: {triggers}\")\n",
    "            \n",
    "        #Check for early stopping\n",
    "        current_loss = val_loss_values[-1]\n",
    "        if current_loss < min_val_loss:\n",
    "            min_val_loss = current_loss\n",
    "            triggers = 0\n",
    "        else:\n",
    "            triggers += 1\n",
    "            total_triggers += 1\n",
    "\n",
    "        if triggers == patience or total_triggers == 20:  #30\n",
    "            return w0,w1,w2,w3,w4,w5,w6,w7,w8,w9,w10,w11,w12,w13, epoch, model,train_loss_values, train_r, train_p, train_AUC, train_MCC, train_labels,train_pred, val_loss_values, val_r, val_p, val_AUC, val_MCC, val_labels,val_pred, test_loss_values, test_r, test_p, test_AUC, test_MCC, test_labels,test_pred\n",
    "        \n",
    "    #Return if model runs through all epochs\n",
    "    return w0,w1,w2,w3,w4,w5,w6,w7,w8,w9,w10,w11,w12,w13, epoch, model, train_loss_values, train_r, train_p, train_AUC, train_MCC, train_labels,train_pred, val_loss_values, val_r, val_p, val_AUC, val_MCC, val_labels,val_pred, test_loss_values, test_r, test_p, test_AUC, test_MCC, test_labels,test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform 5-fold cross validation\n",
    "test_parts = []\n",
    "for fold, (train_idx,val_idx,test_idx) in enumerate(folds):\n",
    "    \n",
    "    #initialize\n",
    "    start = time.time()\n",
    "    fold = fold +1\n",
    "    \n",
    "    #Initialize models folder\n",
    "    if not os.path.exists('./models'):\n",
    "        os.makedirs('./models')\n",
    "    with open(\"./models/logfile.txt\", 'a+') as f:\n",
    "        f.write(f'####  Fold {fold}  ####\\n')\n",
    "    print(f'####  Fold {fold}  ####')\n",
    "    print(f\"Train: {len(train_idx)}\")\n",
    "    print(f\"val: {len(val_idx)}\")\n",
    "    print(f\"Test: {len(test_idx)}\")\n",
    "      \n",
    "    #Get the proper fold\n",
    "    train_X, val_X, test_X = [test_embs[i] for i in train_idx], [test_embs[i] for i in val_idx], [test_embs[i] for i in test_idx]\n",
    "    train_y, val_y, test_y = [test_label[i] for i in train_idx], [test_label[i] for i in val_idx], [test_label[i] for i in test_idx]\n",
    "    train_names = [data_names[i] for i in train_idx]\n",
    "    val_names = [data_names[i] for i in val_idx]\n",
    "    test_names = [data_names[i] for i in test_idx]\n",
    "    test_parts += test_idx\n",
    "    \n",
    "    #Make dataset\n",
    "    train = ProteinDataset(train_X,train_y)\n",
    "    val = ProteinDataset(val_X,val_y)\n",
    "    test = ProteinDataset(test_X,test_y)\n",
    " \n",
    "    #Make data loaders\n",
    "    batch_size = 17 \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, collate_fn=pad_collate, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, collate_fn=pad_collate, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, collate_fn=pad_collate, shuffle=True)\n",
    "    \n",
    "\n",
    "    #Define model\n",
    "    model = Bi_LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, num_classes_nesg = num_classes_nesg, num_classes_psibio = num_classes_psibio, dropout = dropout)\n",
    "    state = torch.load(\"../3_PreTraining/model/model_8/model_conv.state\")\n",
    "    model.load_state_dict(torch.load(\"../3_PreTraining/model/model_8/model_conv.state_dict\"), strict=False)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "\n",
    "     \n",
    "    #Train the model\n",
    "    w0,w1,w2,w3,w4,w5,w6,w7,w8,w9,w10,w11,w12,w13, epoch, model, train_loss_values, train_r, train_p, train_AUC, train_MCC, train_labels,train_pred, val_loss_values, val_r, val_p, val_AUC, val_MCC, val_labels,val_pred, test_loss_values, test_r, test_p, test_AUC, test_MCC, test_labels,test_pred = train_model(model, loss_fn, optimizer, n_epochs)\n",
    "    \n",
    "    #Save in a dictionary\n",
    "    history = {'train_loss': train_loss_values, 'val_loss': val_loss_values,'test_loss': test_loss_values,\n",
    "               'train_AUC':train_AUC,'val_AUC':val_AUC,'test_AUC':test_AUC,\n",
    "               'train_MCC':train_MCC, 'val_MCC':val_MCC, 'test_MCC':test_MCC, \n",
    "               'train_labels':train_labels, 'val_labels': val_labels, 'test_labels': test_labels,\n",
    "               'train_r': train_r, 'train_p': train_p, 'val_r': val_r, 'val_p': val_p,'test_r': test_r, 'test_p': test_p,\n",
    "               'train_pred':train_pred, 'val_pred':val_pred, 'test_pred':test_pred, \n",
    "               'epochs': epoch, 'train_names':train_names, 'val_names':val_names, 'test_names':test_names}\n",
    "    foldperf[f'fold{fold}'] = history\n",
    "    end = time.time()\n",
    "    \n",
    "    #Screen output/logfile\n",
    "    with open(\"./models/logfile.txt\", 'a+') as f:\n",
    "        f.write(f\"---------------- Finished fold {fold} ----------------\\n\")\n",
    "        f.write(f\"Epoch: {epoch} \\nTrain loss: {train_loss_values[-1]} \\t Validation loss: {val_loss_values[-1]} \\t Test loss: {test_loss_values[-1]}\\n\")\n",
    "        f.write(f\"Train AUC: {train_AUC[-1]} \\t Validation AUC: {val_AUC[-1]} \\t Test AUC: {test_AUC[-1]}\\n\")\n",
    "        f.write(f\"Train MCC: {train_MCC[-1]} \\t Validation MCC: { val_MCC[-1]} \\t Test MCC: {test_MCC[-1]}\\n\")\n",
    "        f.write(f\"Train Accurcay: {train_r[-1]} \\t Validation Accurcay: { val_r[-1]} \\t Test Accurcay: { test_r[-1]}\\n\")\n",
    "        f.write(\"Elapsed time: {:.2f} min\\n\".format((end - start)/60))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "#Final save  \n",
    "a_file = open(\"./models/foldperf.pkl\", \"wb\")\n",
    "pickle.dump(foldperf, a_file)\n",
    "a_file.close()\n",
    "print(f\"For testing {len(set(test_parts))} unique protiens were used. A total of {len(test_parts)} proteins\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73809e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import  pickle\n",
    "#pickleFile = open(\"./models_clustering3/foldperf.pkl\", 'rb')\n",
    "#foldperf = pickle.load(pickleFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba660d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "##################\n",
    "#### Make plot ###\n",
    "##################\n",
    "#Average output over epochs\n",
    "import warnings\n",
    "tlf, vlf, ttlf, tpcc, vpcc, ttpcc, tscc, vscc, ttscc, tr, vr, ttr = [],[],[],[],[],[],[],[],[],[],[],[]\n",
    "max_epoch = 0\n",
    "min_epoch = n_epochs\n",
    "for n in range(n_epochs):\n",
    "    tlfe, vlfe, ttlfe, tpcce, vpcce, ttpcce, tscce, vscce, ttscce, tre, vre, ttre = [],[],[],[],[],[],[],[],[],[],[],[]\n",
    "    for f in range(1,11):\n",
    "        try:\n",
    "            max_epoch = (max(max_epoch,foldperf[f'fold{f}']['epochs']))\n",
    "            min_epoch = (min(min_epoch,foldperf[f'fold{f}']['epochs']))\n",
    "            tlfe.append(foldperf[f'fold{f}']['train_loss'][n])\n",
    "            vlfe.append(foldperf[f'fold{f}']['val_loss'][n])\n",
    "            ttlfe.append(foldperf[f'fold{f}']['test_loss'][n])\n",
    "            tpcce.append(foldperf[f'fold{f}']['train_AUC'][n])\n",
    "            vpcce.append(foldperf[f'fold{f}']['val_AUC'][n])\n",
    "            ttpcce.append(foldperf[f'fold{f}']['test_AUC'][n])\n",
    "            tscce.append(foldperf[f'fold{f}']['train_MCC'][n])\n",
    "            vscce.append(foldperf[f'fold{f}']['val_MCC'][n])\n",
    "            ttscce.append(foldperf[f'fold{f}']['test_MCC'][n])\n",
    "            tre.append((foldperf[f'fold{f}']['train_r'][n]))\n",
    "            vre.append((foldperf[f'fold{f}']['val_r'][n]))\n",
    "            ttre.append((foldperf[f'fold{f}']['test_r'][n]))\n",
    "            \n",
    "            \n",
    "        except IndexError as error:\n",
    "            tlfe.append(np.nan)\n",
    "            vlfe.append(np.nan)\n",
    "            ttlfe.append(np.nan)\n",
    "            tpcce.append(np.nan)\n",
    "            vpcce.append(np.nan)\n",
    "            ttpcce.append(np.nan)\n",
    "            tscce.append(np.nan)\n",
    "            vscce.append(np.nan)\n",
    "            ttscce.append(np.nan)\n",
    "            tre.append(np.nan)\n",
    "            vre.append(np.nan)\n",
    "            ttre.append(np.nan)\n",
    "            foldperf[f'fold{f}']['train_loss'] = foldperf[f'fold{f}']['train_loss']+[np.nan]\n",
    "            foldperf[f'fold{f}']['val_loss'] = foldperf[f'fold{f}']['val_loss']+[np.nan]\n",
    "            foldperf[f'fold{f}']['test_loss'] = foldperf[f'fold{f}']['test_loss']+[np.nan]\n",
    "            foldperf[f'fold{f}']['train_AUC'] = foldperf[f'fold{f}']['train_AUC']+[np.nan]\n",
    "            foldperf[f'fold{f}']['val_AUC'] = foldperf[f'fold{f}']['val_AUC'] +[np.nan]\n",
    "            foldperf[f'fold{f}']['test_AUC'] = foldperf[f'fold{f}']['test_AUC']+[np.nan]\n",
    "            foldperf[f'fold{f}']['train_MCC'] = foldperf[f'fold{f}']['train_MCC']+[np.nan]\n",
    "            foldperf[f'fold{f}']['val_MCC']  = foldperf[f'fold{f}']['val_MCC'] +[np.nan]\n",
    "            foldperf[f'fold{f}']['test_MCC'] = foldperf[f'fold{f}']['test_MCC']+[np.nan]\n",
    "            foldperf[f'fold{f}']['train_r']  = foldperf[f'fold{f}']['train_r'] +[np.nan]\n",
    "            foldperf[f'fold{f}']['val_r']  = foldperf[f'fold{f}']['val_r'] +[np.nan]\n",
    "            foldperf[f'fold{f}']['test_r']  = foldperf[f'fold{f}']['test_r'] +[np.nan]\n",
    "            \n",
    "    # Catch warnings for making means with nan        \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        tlf.append(np.nanmean(tlfe))\n",
    "        vlf.append(np.nanmean(vlfe))\n",
    "        ttlf.append(np.nanmean(ttlfe))\n",
    "        tpcc.append(np.nanmean(tpcce))\n",
    "        vpcc.append(np.nanmean(vpcce))\n",
    "        ttpcc.append(np.nanmean(ttpcce))\n",
    "        tscc.append(np.nanmean(tscce))\n",
    "        vscc.append(np.nanmean(vscce))\n",
    "        ttscc.append(np.nanmean(ttscce))\n",
    "        tr.append(np.nanmean(tre))\n",
    "        vr.append(np.nanmean(vre))\n",
    "        ttr.append(np.nanmean(ttre))\n",
    "\n",
    "#Get only the relevant length (max_epoch)        \n",
    "tlf = tlf[:max_epoch]\n",
    "vlf = vlf[:max_epoch]\n",
    "ttlf = ttlf[:max_epoch]\n",
    "tpcc = tpcc[:max_epoch]\n",
    "vpcc = vpcc[:max_epoch]\n",
    "ttpcc = ttpcc[:max_epoch]\n",
    "tscc = tscc[:max_epoch]\n",
    "vscc = vscc[:max_epoch]\n",
    "ttscc = ttscc[:max_epoch]\n",
    "tr = tr[:max_epoch]\n",
    "vr = vr[:max_epoch]\n",
    "ttr = ttr[:max_epoch]\n",
    "\n",
    "for f in range(1,11):\n",
    "    foldperf[f'fold{f}']['train_loss'] = foldperf[f'fold{f}']['train_loss'][:max_epoch]\n",
    "    foldperf[f'fold{f}']['val_loss'] = foldperf[f'fold{f}']['val_loss'][:max_epoch]\n",
    "    foldperf[f'fold{f}']['test_loss'] = foldperf[f'fold{f}']['test_loss'][:max_epoch]\n",
    "    foldperf[f'fold{f}']['train_AUC'] = foldperf[f'fold{f}']['train_AUC'][:max_epoch]\n",
    "    foldperf[f'fold{f}']['val_AUC'] = foldperf[f'fold{f}']['val_AUC'][:max_epoch]\n",
    "    foldperf[f'fold{f}']['test_AUC'] = foldperf[f'fold{f}']['test_AUC'][:max_epoch]\n",
    "    foldperf[f'fold{f}']['train_MCC'] = foldperf[f'fold{f}']['train_MCC'][:max_epoch]\n",
    "    foldperf[f'fold{f}']['val_MCC']  = foldperf[f'fold{f}']['val_MCC'][:max_epoch]\n",
    "    foldperf[f'fold{f}']['test_MCC'] = foldperf[f'fold{f}']['test_MCC'][:max_epoch]\n",
    "    foldperf[f'fold{f}']['train_r']  = foldperf[f'fold{f}']['train_r'][:max_epoch]\n",
    "    foldperf[f'fold{f}']['val_r']  = foldperf[f'fold{f}']['val_r'][:max_epoch]\n",
    "    foldperf[f'fold{f}']['test_r']  = foldperf[f'fold{f}']['test_r'][:max_epoch]\n",
    "\n",
    "    \n",
    "#Make pretty plot\n",
    "plt.rcParams['figure.figsize'] = [25, 25]   \n",
    "plt.rcParams['font.size']=20\n",
    "\n",
    "#Initialize plot\n",
    "fig, ((ax1, ax3), (ax2,ax4)) = plt.subplots(2, 2)\n",
    "fig.patch.set_facecolor('#FAFAFA')\n",
    "fig.patch.set_alpha(0.7)\n",
    "x = list(range(1,max_epoch+1))\n",
    "\n",
    "#base * round(a_number/base)\n",
    "\n",
    "###### Plot loss ######\n",
    "ax1.plot(x,vlf, label = \"Average loss of Validation data\", c=\"blue\", lw = 5 )\n",
    "ax1.plot(x,tlf, label = \"Average loss of Training data\", c=\"red\", lw = 5)\n",
    "ax1.plot(x,ttlf, label = \"Average loss of Testing data\", c=\"cornflowerblue\", lw = 5)\n",
    "ax1.axvline(min_epoch,ls = '--', c = \"grey\", label = \"First early stopping of a fold\", lw = 3)\n",
    "\n",
    "# Add each fold\n",
    "ax1.plot(x,foldperf[f'fold1']['train_loss'], label = \"Fold 1-10 Training data\", c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold2']['train_loss'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold3']['train_loss'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold4']['train_loss'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold5']['train_loss'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold6']['train_loss'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold7']['train_loss'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold8']['train_loss'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold9']['train_loss'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold10']['train_loss'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold1']['val_loss'], label = \"Fold 1-10 Validation data\", c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold2']['val_loss'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold3']['val_loss'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold4']['val_loss'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold5']['val_loss'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold6']['val_loss'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold7']['val_loss'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold8']['val_loss'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold9']['val_loss'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold10']['val_loss'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold1']['test_loss'], label = \"Fold 1-10 Test data\", c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold2']['test_loss'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold3']['test_loss'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold4']['test_loss'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold5']['test_loss'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold6']['test_loss'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold7']['test_loss'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold8']['test_loss'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold9']['test_loss'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax1.plot(x,foldperf[f'fold10']['test_loss'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "\n",
    "# Make plot pretty\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Training loss')\n",
    "ax1.set_title(\"Loss curve\")\n",
    "ax1.legend(loc='upper center')\n",
    "#ax1.set_xticks(np.arange(1,max_epoch+1,1))\n",
    "#ax1.set_yticks(np.arange(0.26,0.37,0.02))\n",
    "ax1.grid(True)\n",
    "\n",
    "##### Plot AUC ####\n",
    "ax2.plot(x,vpcc, label = \"Average AUC of Validation data\", c = \"blue\", lw = 5)\n",
    "ax2.plot(x,tpcc, label = \"Average AUC of Training data\", c = \"red\", lw = 5)\n",
    "ax2.plot(x,ttpcc, label = \"Average AUC of Testing data\", c = \"cornflowerblue\", lw = 5)\n",
    "ax2.axvline(min_epoch,ls = '--', c = \"grey\", label = \"First early stopping of a fold\", lw = 3)\n",
    "# Add each fold\n",
    "ax2.plot(x,foldperf[f'fold1']['train_AUC'], label = \"Fold 1-10 Training data\", c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold2']['train_AUC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold3']['train_AUC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold4']['train_AUC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold5']['train_AUC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold6']['train_AUC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold7']['train_AUC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold8']['train_AUC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold9']['train_AUC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold10']['train_AUC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold1']['val_AUC'], label = \"Fold 1-10 Validation data\", c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold2']['val_AUC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold3']['val_AUC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold4']['val_AUC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold5']['val_AUC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold6']['val_AUC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold7']['val_AUC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold8']['val_AUC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold9']['val_AUC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold10']['val_AUC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold1']['test_AUC'], label = \"Fold 1-10 Test data\", c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold2']['test_AUC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold3']['test_AUC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold4']['test_AUC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold5']['test_AUC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold6']['test_AUC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold7']['test_AUC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold8']['test_AUC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold9']['test_AUC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax2.plot(x,foldperf[f'fold10']['test_AUC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "\n",
    "\n",
    "\n",
    "# Make plot pretty\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('AUC')\n",
    "ax2.set_title(\"AUC\")\n",
    "ax2.legend(loc='lower right')\n",
    "#ax2.set_xticks(np.arange(1,max_epoch+1,1))\n",
    "#ax2.set_yticks(np.arange(0.5,0.7,0.05))\n",
    "ax2.grid(True)\n",
    "\n",
    "    \n",
    "#### Plot MCC ####\n",
    "ax3.plot(x,vscc, label = \"Average MCC of Validation data\", c = \"blue\", lw = 5)\n",
    "ax3.plot(x,tscc, label = \"Average MCC of Training data\", lw = 5, c=\"red\")\n",
    "ax3.plot(x,ttscc, label = \"Average MCC of Testing data\", lw = 5, c=\"cornflowerblue\")\n",
    "ax3.axvline(min_epoch,ls = '--', c = \"grey\", label = \"First early stopping of a fold\", lw = 3)\n",
    "# Add each fold\n",
    "ax3.plot(x,foldperf[f'fold1']['train_MCC'], label = \"Fold 1-10 Training data\", c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold2']['train_MCC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold3']['train_MCC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold4']['train_MCC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold5']['train_MCC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold6']['train_MCC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold7']['train_MCC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold8']['train_MCC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold9']['train_MCC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold10']['train_MCC'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold1']['val_MCC'], label = \"Fold 1-10 Validation data\", c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold2']['val_MCC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold3']['val_MCC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold4']['val_MCC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold5']['val_MCC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold6']['val_MCC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold7']['val_MCC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold8']['val_MCC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold9']['val_MCC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold10']['val_MCC'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold1']['test_MCC'], label = \"Fold 1-10 Test data\", c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold2']['test_MCC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold3']['test_MCC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold4']['test_MCC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold5']['test_MCC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold6']['test_MCC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold7']['test_MCC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold8']['test_MCC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold9']['test_MCC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax3.plot(x,foldperf[f'fold10']['test_MCC'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "\n",
    "# Make plot pretty\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('MCC')\n",
    "ax3.set_title(\"MCC\")\n",
    "ax3.legend(loc='lower right')\n",
    "#ax3.set_xticks(np.arange(1,max_epoch+1,1))\n",
    "#ax3.set_yticks(np.arange(0,0.6,0.1))\n",
    "ax3.grid(True)\n",
    "\n",
    "\n",
    "#### Plot Accuracy ####\n",
    "ax4.plot(x,vr, label = \"Average Accurcay of Validation data\", c = \"blue\", lw = 5)\n",
    "ax4.plot(x,tr, label = \"Average Accurcay of Training data\", lw = 5, c=\"red\")\n",
    "ax4.plot(x,ttr, label = \"Average Accurcay of Testing data\", lw = 5, c=\"cornflowerblue\")\n",
    "ax4.axvline(min_epoch,ls = '--', c = \"grey\", label = \"First early stopping of a fold\", lw = 3)\n",
    "# Add each fold\n",
    "ax4.plot(x,foldperf[f'fold1']['train_r'], label = \"Fold 1-10 Training data\", c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold2']['train_r'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold3']['train_r'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold4']['train_r'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold5']['train_r'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold6']['train_r'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold7']['train_r'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold8']['train_r'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold9']['train_r'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold10']['train_r'], c=\"palevioletred\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold1']['val_r'], label = \"Fold 1-10 Validation data\", c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold2']['val_r'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold3']['val_r'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold4']['val_r'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold5']['val_r'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold6']['val_r'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold7']['val_r'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold8']['val_r'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold9']['val_r'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold10']['val_r'], c=\"cornflowerblue\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold1']['test_r'], label = \"Fold 1-10 Test data\", c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold2']['test_r'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold3']['test_r'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold4']['test_r'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold5']['test_r'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold6']['test_r'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold7']['test_r'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold8']['test_r'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold9']['test_r'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "ax4.plot(x,foldperf[f'fold10']['test_r'], c=\"plum\", lw = 3, alpha = 0.2)\n",
    "\n",
    "# Make plot pretty\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Accurcay')\n",
    "ax4.set_title(\"Accurcay\")\n",
    "ax4.legend(loc='lower right')\n",
    "#ax4.set_xticks(np.arange(1,max_epoch+1,1))\n",
    "#ax3.set_yticks(np.arange(0,0.6,0.1))\n",
    "ax4.grid(True)\n",
    "\n",
    "\n",
    "fig.tight_layout(pad = 1)\n",
    "fig.savefig(f'./models/Loss_AUC_MCC_pretty.png', facecolor=fig.get_facecolor(), edgecolor='none')\n",
    "#plt.show()\n",
    "\n",
    "#save_model(\"./final\", n_epochs, model, train_loss, val_loss, train_nesg_PCC,  val_nesg_PCC, train_psibio_MCC, val_psibio_MCC, train_psibio_AUC, val_psibio_AUC, labels_out, predictions_out)   \n",
    "#plot_performance(\"./final\",n_epochs,train_loss, val_loss,train_psibio_AUC,val_psibio_AUC,train_nesg_PCC,val_nesg_PCC,train_psibio_MCC,val_psibio_MCC)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a64cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get specific last output\n",
    "print(f\"Test Loss: {ttlf[-1]}\")\n",
    "print(f\"Test AUC: {ttpcc[-1]}\")\n",
    "print(f\"Test MCC: {ttscc[-1]}\")\n",
    "print(f\"Test Accuracy: {ttr[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eda1bb",
   "metadata": {},
   "source": [
    "## Check if the weights are updated throughout training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe3050",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_w = [w0,w1,w2,w3,w4,w5,w6,w7,w8,w9,w10,w11,w12,w13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c3a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare weights between first and last epoch\n",
    "total = 0\n",
    "static = 0\n",
    "cause = []\n",
    "count = 0\n",
    "for i in all_w:\n",
    "    count += 1\n",
    "    eq = torch.eq(i[0], i[-1])\n",
    "    summa = sum(eq)\n",
    "    total += len(i[0])\n",
    "    summa = sum(summa)\n",
    "    static += summa\n",
    "    if summa > 0:\n",
    "        cause.append(f\"w{count}\")\n",
    "        \n",
    "        \n",
    "print(f\"{static} static weights out of {total} total weights\")\n",
    "print(f\"Causes: {cause}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d4e3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc4455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
